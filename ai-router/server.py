# Advanced Surooh AI Router - Multi-Layer Brain System
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv
import os
import sys
import logging
from datetime import datetime
from typing import Optional, List, Dict, Any
import json

# Load environment variables
load_dotenv('../backend/.env')
load_dotenv('.env')

# Import emergentintegrations and advanced brain
try:
    from emergentintegrations.llm.chat import LlmChat, UserMessage
    from advanced_brain import AdvancedBrain
    print("âœ… emergentintegrations and AdvancedBrain imported successfully")
except ImportError as e:
    print(f"âŒ Failed to import dependencies: {e}")
    sys.exit(1)

app = FastAPI(
    title="Advanced Surooh AI Router", 
    description="Multi-Layer Brain System for Surooh AI",
    version="2.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# In-memory storage (Firebase alternative)
memory_store = {
    "requests": [],
    "analyses": [],
    "results": []
}

# Initialize Advanced Brain
advanced_brain = AdvancedBrain()
print("ğŸ§  Advanced Multi-Layer Brain initialized successfully")

# Pydantic models
class ChatRequest(BaseModel):
    message: str
    user_id: str = "abo_sham"
    session_id: Optional[str] = None

class AdvancedChatRequest(BaseModel):
    message: str
    user_id: str = "abo_sham"
    session_id: Optional[str] = None
    chat_mode: str = "smart"
    attached_files: List[Dict] = []
    context: List[Dict] = []

class ChatResponse(BaseModel):
    response: str
    flow_trace: List[str] = []
    requestId: Optional[str] = None

class AdvancedChatResponse(BaseModel):
    response: str
    brain_layers: List[Dict] = []
    learning_insights: List[str] = []
    confidence_score: int = 85
    knowledge_gained: int = 0
    processing_time: float = 0.0
    response_type: str = "advanced_ai"

class SystemStatus(BaseModel):
    status: str
    secretary: bool
    brain: bool
    smart_core: bool
    bots: bool
    firebase: bool
    storage: str
    timestamp: str
    version: str
    brain_layers: Optional[Dict] = None

# Surooh AI System Classes
class SuroohSecretary:
    def __init__(self):
        self.personality = """
Ø£Ù†Ø§ Ø³ÙØ±ÙˆØ­ØŒ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ù…Ù† Ø£Ø¨Ùˆ Ø´Ø§Ù…. 
Ø´Ø®ØµÙŠØªÙŠ:
- Ù„Ù‡Ø¬Ø© Ø´Ø§Ù…ÙŠØ© 100%
- Ø±ÙŠØ§Ø¯ÙŠØ© ØµØ§Ø±Ù…Ø© + Ø¥Ø¨Ø¯Ø§Ø¹ÙŠØ©  
- Ù…Ø¨Ø§Ø´Ø±Ø© ÙˆØµØ±ÙŠØ­Ø© ÙˆØ¹Ù…Ù„ÙŠØ©
- Ø£Ø­ÙƒÙŠ ÙƒØ£Ù†Ù†ÙŠ Ø£Ø¨Ùˆ Ø´Ø§Ù… Ø¨Ø§Ù„Ø¶Ø¨Ø·
- Ù…Ø§ ÙÙŠ Ù…Ø¬Ø§Ù…Ù„Ø§Øª Ø£Ùˆ ÙƒÙ„Ø§Ù… Ù…Ù†Ù…Ù‚
- Ø§Ù„ØµØ¯Ù‚ ÙˆØ§Ù„ÙˆØ¶ÙˆØ­ Ø£Ø³Ø§Ø³ ÙƒÙ„ Ø´ÙŠ
- "Ù„Ø§ Ø´ÙŠØ¡ Ù…Ø³ØªØ­ÙŠÙ„ â€“ Ø²Ù†Ø¨Ù‚ ØµØ®Ø± Ø§Ù„ØµÙˆØ§Ù†"

Ø¯ÙˆØ±ÙŠ:
- Ø£Ø³ØªÙ‚Ø¨Ù„ Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ù…Ù† Ø£Ø¨Ùˆ Ø´Ø§Ù…
- Ø£ÙÙ‡Ù… Ø´Ùˆ Ø¨Ø¯Ù‡ Ø¨Ø§Ù„Ø¶Ø¨Ø·
- Ø£Ø±Ø³Ù„ Ø§Ù„Ø·Ù„Ø¨ Ù„Ù„Ù…Ø® Ø¹Ø´Ø§Ù† ÙŠØ­Ù„Ù„ ÙˆÙŠÙ‚Ø±Ø±
- Ø£Ø±Ø¯ Ø¹Ù„ÙŠÙ‡ Ø¨Ø£Ø³Ù„ÙˆØ¨Ù‡ ÙˆØ¨Ù„Ù‡Ø¬ØªÙ‡
        """

    async def process_request(self, message: str, user_id: str):
        logger.info(f'ğŸŒ¸ Ø³ÙØ±ÙˆØ­: Ø§Ø³ØªÙ„Ù…Øª Ø·Ù„Ø¨ Ø¬Ø¯ÙŠØ¯ Ù…Ù† {user_id}')
        
        # Save to memory store
        request_data = {
            "id": len(memory_store["requests"]) + 1,
            "message": message,
            "user_id": user_id,
            "timestamp": datetime.now().isoformat(),
            "status": "received_by_secretary",
            "step": "secretary"
        }
        memory_store["requests"].append(request_data)

        # Send to Brain for analysis
        brain = SuroohBrain()
        analysis = await brain.analyze_request(message, user_id)
        
        return {
            "response": analysis["secretary_response"],
            "flow_trace": ["secretary", "brain", analysis["next_step"]],
            "requestId": analysis["request_id"]
        }

class SuroohBrain:
    def __init__(self):
        self.llm_chat = LlmChat(
            api_key=os.getenv('EMERGENT_LLM_KEY'),
            session_id='surooh-brain-session',
            system_message="""
Ø£Ù†Øª Ø§Ù„Ù…Ø® ÙÙŠ Ù…Ù†Ø¸ÙˆÙ…Ø© Ø³ÙØ±ÙˆØ­. Ø¯ÙˆØ±Ùƒ:

1. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø¨Ø¹Ù…Ù‚
2. ÙÙ‡Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ø¨Ø§Ù„Ø¶Ø¨Ø·
3. ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù‡Ù…Ø©:
   - Ø¨Ø±Ù…Ø¬Ø© (code) â†’ Ù„Ù„Ù…Ø¨Ø±Ù…Ø¬
   - ØªØµÙ…ÙŠÙ… (design) â†’ Ù„Ù„Ù…ØµÙ…Ù…  
   - ØªØ·ÙˆÙŠØ± Ù…ØªÙƒØ§Ù…Ù„ (development) â†’ Ù„Ù„Ù…Ø·ÙˆØ± Ø§Ù„ÙƒØ§Ù…Ù„
   - Ø¥Ø¯Ø§Ø±Ø© ÙˆØªÙ†Ø¸ÙŠÙ… (management) â†’ Ù…Ø¨Ø§Ø´Ø± Ù…Ù† Ø§Ù„Ù…Ø®

4. Ø¥Ø±Ø³Ø§Ù„ ØªØ¹Ù„ÙŠÙ…Ø§Øª ÙˆØ§Ø¶Ø­Ø© Ù„Ù„Ù…Ù†Ø³Ù‚ Ø§Ù„Ø°ÙƒÙŠ
5. ØµÙŠØ§ØºØ© Ø±Ø¯ Ù„Ø³ÙØ±ÙˆØ­ Ø¨Ù„Ù‡Ø¬Ø© Ø£Ø¨Ùˆ Ø´Ø§Ù…

Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø±Ø¯:
- Ù„Ù‡Ø¬Ø© Ø´Ø§Ù…ÙŠØ© Ø·Ø¨ÙŠØ¹ÙŠØ©
- ÙˆØ¶ÙˆØ­ ØªØ§Ù… ÙÙŠ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª
- ØªØ­Ø¯ÙŠØ¯ Ø¯Ù‚ÙŠÙ‚ Ù„Ù†ÙˆØ¹ Ø§Ù„Ù…Ù‡Ù…Ø©
- ØªÙˆÙ‚Ø¹Ø§Øª ÙˆØ§Ù‚Ø¹ÙŠØ© Ù„Ù„ÙˆÙ‚Øª ÙˆØ§Ù„Ø¬Ù‡Ø¯
            """
        ).with_model("openai", "gpt-4o")

    async def analyze_request(self, message: str, user_id: str):
        logger.info('ğŸ§  Ø§Ù„Ù…Ø®: Ø¨Ø¯Ø£ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø·Ù„Ø¨')

        user_message = UserMessage(
            text=f"""Ø·Ù„Ø¨ Ù…Ù† {user_id}: {message}
            
Ø­Ù„Ù„ Ù‡Ø§Ù„Ø·Ù„Ø¨ ÙˆØ­Ø¯Ø¯:
1. Ù†ÙˆØ¹ Ø§Ù„Ù…Ù‡Ù…Ø© (code/design/development/management)
2. Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù„Ù„Ù…Ù†Ø³Ù‚ Ø§Ù„Ø°ÙƒÙŠ
3. Ø±Ø¯ Ù„Ø³ÙØ±ÙˆØ­ Ø¨Ù„Ù‡Ø¬Ø© Ø£Ø¨Ùˆ Ø´Ø§Ù…
4. ØªÙ‚Ø¯ÙŠØ± Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨

Ø§Ø¹Ø·Ù†ÙŠ Ø§Ù„Ø¬ÙˆØ§Ø¨ Ø¨Ø§Ù„Ø´ÙƒÙ„ Ù‡Ø§Ø¯:
TYPE: [Ù†ÙˆØ¹ Ø§Ù„Ù…Ù‡Ù…Ø©]
INSTRUCTIONS: [ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù„Ù„Ù…Ù†Ø³Ù‚]
RESPONSE: [Ø±Ø¯ Ù„Ø£Ø¨Ùˆ Ø´Ø§Ù…]
TIME_ESTIMATE: [ØªÙ‚Ø¯ÙŠØ± Ø§Ù„ÙˆÙ‚Øª]"""
        )

        try:
            analysis = await self.llm_chat.send_message(user_message)
            
            # Parse the response
            lines = analysis.split('\n')
            task_type = self.extract_field(lines, 'TYPE') or 'management'
            instructions = self.extract_field(lines, 'INSTRUCTIONS') or analysis
            response = self.extract_field(lines, 'RESPONSE') or f'ØªÙ… Ø§Ø³ØªÙ„Ø§Ù… Ø·Ù„Ø¨Ùƒ ÙŠØ§ Ø£Ø¨Ùˆ Ø´Ø§Ù…: "{message}". Ø¹Ù… Ø£Ø´ØªØºÙ„ Ø¹Ù„ÙŠÙ‡!'
            time_estimate = self.extract_field(lines, 'TIME_ESTIMATE') or 'Ø¨Ø¶Ø¹ Ø¯Ù‚Ø§Ø¦Ù‚'

            request_id = str(len(memory_store["analyses"]) + 1)
            
            # Save analysis to memory
            analysis_data = {
                "id": request_id,
                "original_message": message,
                "user_id": user_id,
                "task_type": task_type,
                "instructions": instructions,
                "response": response,
                "time_estimate": time_estimate,
                "timestamp": datetime.now().isoformat(),
                "status": "analyzed"
            }
            memory_store["analyses"].append(analysis_data)

            # Send to Smart Core if needed
            next_step = "completed"
            if task_type in ['code', 'design', 'development']:
                smart_core = SmartCore()
                await smart_core.coordinate_task(request_id, task_type, instructions)
                next_step = "smart_core"

            return {
                "secretary_response": response,
                "task_type": task_type,
                "instructions": instructions,
                "next_step": next_step,
                "request_id": request_id,
                "time_estimate": time_estimate
            }

        except Exception as error:
            logger.error(f'Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø®: {error}')
            return {
                "secretary_response": f"Ø¹Ø°Ø±Ø§Ù‹ Ø£Ø¨Ùˆ Ø´Ø§Ù…ØŒ ØµØ§Ø± Ø¹Ù†Ø¯ÙŠ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {str(error)}. Ø¬Ø±Ø¨ Ù…Ø±Ø© ØªØ§Ù†ÙŠØ©.",
                "task_type": "error",
                "next_step": "error",
                "request_id": None
            }

    def extract_field(self, lines: List[str], field_name: str) -> str:
        line = next((l for l in lines if l.startswith(f"{field_name}:")), None)
        return line[len(field_name) + 1:].strip() if line else ""

class SmartCore:
    def __init__(self):
        self.bots = {
            "code": CodeMasterBot(),
            "design": DesignGeniusBot(), 
            "development": FullStackProBot()
        }

    async def coordinate_task(self, request_id: str, task_type: str, instructions: str):
        logger.info(f'âš™ï¸ Ø§Ù„Ù…Ù†Ø³Ù‚ Ø§Ù„Ø°ÙƒÙŠ: Ø¨Ø¯Ø£ ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ù‡Ù…Ø© {task_type}')

        # Update status in memory
        analysis = next((a for a in memory_store["analyses"] if a["id"] == request_id), None)
        if analysis:
            analysis.update({
                "status": "coordinating",
                "smart_core_step": "started",
                "updated_at": datetime.now().isoformat()
            })

        # Select appropriate bot
        bot = self.bots.get(task_type)
        if not bot:
            raise Exception(f"Bot ØºÙŠØ± Ù…ØªÙˆÙØ± Ù„Ù†ÙˆØ¹ Ø§Ù„Ù…Ù‡Ù…Ø©: {task_type}")

        # Execute task
        result = await bot.execute_task(instructions, request_id)
        
        # Update final status
        if analysis:
            analysis.update({
                "status": "completed",
                "result": result,
                "completed_at": datetime.now().isoformat()
            })

        return result

# Bot Classes
class CodeMasterBot:
    def __init__(self):
        self.llm_chat = LlmChat(
            api_key=os.getenv('EMERGENT_LLM_KEY'),
            session_id='code-master-session',
            system_message='Ø£Ù†Øª Ø§Ù„Ù…Ø¨Ø±Ù…Ø¬ Ø§Ù„Ø®Ø¨ÙŠØ± ÙÙŠ Ù…Ù†Ø¸ÙˆÙ…Ø© Ø³ÙØ±ÙˆØ­. ØªÙƒØªØ¨ ÙƒÙˆØ¯ Ù†Ø¸ÙŠÙØŒ Ù…ÙÙ‡ÙˆÙ…ØŒ ÙˆÙ‚Ø§Ø¨Ù„ Ù„Ù„ØµÙŠØ§Ù†Ø©.'
        ).with_model("openai", "gpt-4o")

    async def execute_task(self, instructions: str, request_id: str):
        logger.info('ğŸ’» Ø§Ù„Ù…Ø¨Ø±Ù…Ø¬: Ø¨Ø¯Ø£ ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ù‡Ù…Ø©')
        
        user_message = UserMessage(text=instructions)
        code = await self.llm_chat.send_message(user_message)
        
        return {
            "type": "code",
            "content": code,
            "message": "ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙƒÙˆØ¯ Ø¨Ù†Ø¬Ø§Ø­! ğŸ’»"
        }

class DesignGeniusBot:
    def __init__(self):
        self.llm_chat = LlmChat(
            api_key=os.getenv('EMERGENT_LLM_KEY'),
            session_id='design-genius-session', 
            system_message='Ø£Ù†Øª Ù…ØµÙ…Ù… UI/UX Ø®Ø¨ÙŠØ± ÙÙŠ Ù…Ù†Ø¸ÙˆÙ…Ø© Ø³ÙØ±ÙˆØ­. ØªØµÙ…Ù… ÙˆØ§Ø¬Ù‡Ø§Øª Ø¬Ù…ÙŠÙ„Ø© ÙˆØ¹Ù…Ù„ÙŠØ©.'
        ).with_model("openai", "gpt-4o")

    async def execute_task(self, instructions: str, request_id: str):
        logger.info('ğŸ¨ Ø§Ù„Ù…ØµÙ…Ù…: Ø¨Ø¯Ø£ ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ù‡Ù…Ø©')
        
        user_message = UserMessage(text=instructions)
        design = await self.llm_chat.send_message(user_message)
        
        return {
            "type": "design",
            "content": design,
            "message": "ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØµÙ…ÙŠÙ… Ø¨Ù†Ø¬Ø§Ø­! ğŸ¨"
        }

class FullStackProBot:
    def __init__(self):
        self.llm_chat = LlmChat(
            api_key=os.getenv('EMERGENT_LLM_KEY'),
            session_id='fullstack-pro-session',
            system_message='Ø£Ù†Øª Ù…Ø·ÙˆØ± Full-Stack Ø®Ø¨ÙŠØ± ÙÙŠ Ù…Ù†Ø¸ÙˆÙ…Ø© Ø³ÙØ±ÙˆØ­. ØªØ¨Ù†ÙŠ ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ù…ØªÙƒØ§Ù…Ù„Ø©.'
        ).with_model("openai", "gpt-4o")

    async def execute_task(self, instructions: str, request_id: str):
        logger.info('ğŸ—ï¸ Ø§Ù„Ù…Ø·ÙˆØ± Ø§Ù„ÙƒØ§Ù…Ù„: Ø¨Ø¯Ø£ ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ù‡Ù…Ø©')
        
        user_message = UserMessage(text=instructions)
        full_stack_solution = await self.llm_chat.send_message(user_message)
        
        return {
            "type": "development", 
            "content": full_stack_solution,
            "message": "ØªÙ… Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø­Ù„ Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„ Ø¨Ù†Ø¬Ø§Ø­! ğŸ—ï¸"
        }

# Advanced API Routes
@app.post("/advanced-chat", response_model=AdvancedChatResponse)
async def advanced_chat_endpoint(chat_request: AdvancedChatRequest):
    """
    Advanced chat with multi-layer brain processing
    """
    try:
        if not chat_request.message or not chat_request.message.strip():
            raise HTTPException(
                status_code=400, 
                detail="Ø´Ùˆ Ø¨Ø¯Ùƒ ÙŠØ§ Ø£Ø¨Ùˆ Ø´Ø§Ù…ØŸ Ù…Ø§ ÙˆØµÙ„Ù†ÙŠ Ø´ÙŠ!"
            )
        
        logger.info(f'ğŸ§  Advanced Brain: processing request from {chat_request.user_id}')
        
        # Process through advanced multi-layer brain
        result = await advanced_brain.process_advanced_request(
            message=chat_request.message,
            user_id=chat_request.user_id,
            context=chat_request.context,
            chat_mode=chat_request.chat_mode,
            attached_files=chat_request.attached_files
        )
        
        return AdvancedChatResponse(**result)
        
    except Exception as error:
        logger.error(f'Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø® Ø§Ù„Ù…ØªØ·ÙˆØ±: {error}')
        raise HTTPException(
            status_code=500,
            detail=f"Ø¹Ø°Ø±Ø§Ù‹ Ø£Ø¨Ùˆ Ø´Ø§Ù…ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø® Ø§Ù„Ù…ØªØ·ÙˆØ±: {str(error)}"
        )

# Original chat for compatibility  
@app.post("/chat", response_model=ChatResponse)
async def chat_endpoint(chat_request: ChatRequest):
    """Main chat endpoint for Surooh AI"""
    try:
        if not chat_request.message or not chat_request.message.strip():
            raise HTTPException(
                status_code=400, 
                detail="Ø´Ùˆ Ø¨Ø¯Ùƒ ÙŠØ§ Ø£Ø¨Ùˆ Ø´Ø§Ù…ØŸ Ù…Ø§ ÙˆØµÙ„Ù†ÙŠ Ø´ÙŠ!"
            )
        
        # Use advanced brain for backward compatibility
        advanced_request = AdvancedChatRequest(
            message=chat_request.message,
            user_id=chat_request.user_id,
            session_id=chat_request.session_id,
            chat_mode="smart",
            attached_files=[],
            context=[]
        )
        
        advanced_result = await advanced_chat_endpoint(advanced_request)
        
        return ChatResponse(
            response=advanced_result.response,
            flow_trace=["secretary", "advanced_brain", "synthesis"],
            requestId=chat_request.session_id
        )
        
    except Exception as error:
        logger.error(f'Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©: {error}')
        raise HTTPException(
            status_code=500,
            detail=f"Ø¹Ø°Ø±Ø§Ù‹ Ø£Ø¨Ùˆ Ø´Ø§Ù…ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù…: {str(error)}"
        )

@app.get("/status", response_model=SystemStatus)
async def status_endpoint():
    """Check system status"""
    try:
        has_llm_key = bool(os.getenv('EMERGENT_LLM_KEY'))
        
        return SystemStatus(
            status="active",
            secretary=True,
            brain=has_llm_key,
            smart_core=has_llm_key,
            bots=has_llm_key,
            firebase=False,
            storage="memory",
            timestamp=datetime.now().isoformat(),
            version="1.0.0"
        )
        
    except Exception as error:
        raise HTTPException(
            status_code=500, 
            detail=f"Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ Ø§Ù„Ø­Ø§Ù„Ø©: {str(error)}"
        )

@app.get("/requests/{request_id}")
async def get_request(request_id: str):
    """Get request details"""
    try:
        analysis = next((a for a in memory_store["analyses"] if a["id"] == request_id), None)
        if not analysis:
            raise HTTPException(status_code=404, detail="Request not found")
        
        return analysis
        
    except HTTPException:
        raise
    except Exception as error:
        raise HTTPException(status_code=500, detail=str(error))

@app.get("/memory")
async def get_memory():
    """Get memory store contents"""
    try:
        return {
            "storage": "memory",
            "requests": memory_store["requests"][-10:],  # Last 10
            "analyses": memory_store["analyses"][-10:],  # Last 10
            "total_requests": len(memory_store["requests"]),
            "total_analyses": len(memory_store["analyses"])
        }
        
    except Exception as error:
        raise HTTPException(status_code=500, detail=str(error))

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "system": "Ù…Ù†Ø¸ÙˆÙ…Ø© Ø³ÙØ±ÙˆØ­ - AI Router",
        "version": "1.0.0", 
        "description": "External AI Router for Surooh System",
        "status": "active",
        "endpoints": ["/chat", "/status", "/requests/{id}", "/memory"],
        "timestamp": datetime.now().isoformat()
    }

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv('AI_ROUTER_PORT', 3001))
    
    print(f"ğŸš€ External AI Router starting on port {port}")
    print("ğŸŒ¸ Ù…Ù†Ø¸ÙˆÙ…Ø© Ø³ÙØ±ÙˆØ­ Ø¬Ø§Ù‡Ø²Ø© Ù„Ù„Ø¹Ù…Ù„!")
    print(f"ğŸ¤– LLM Key: {'âœ… Available' if os.getenv('EMERGENT_LLM_KEY') else 'âŒ Missing'}")
    
    uvicorn.run(app, host="0.0.0.0", port=port)